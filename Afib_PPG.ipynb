{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Afib_PPG.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMAMiGRhSeX9tICyDS1yUy9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/supertime1/Afib_PPG/blob/master/Afib_PPG.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uWP-_pXlUKmp",
        "colab_type": "text"
      },
      "source": [
        "#1.Introduction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3mOoV1yuUKlh",
        "colab_type": "text"
      },
      "source": [
        "This notebook trains an PPG DNN by using labeled PPG data from Afib_Data_Clean notebook;\n",
        "The loaded data is 30s segemented PPG signals with 125Hz sampling rate."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bfCfr60kSTDU",
        "colab_type": "text"
      },
      "source": [
        "#2.Setup Environment\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cvqxVijHSFk2",
        "colab_type": "code",
        "outputId": "7aef8a6a-553f-43d1-8408-9577d449480b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "source": [
        "from IPython.display import display\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "%load_ext tensorboard\n",
        "import numpy as np\n",
        "import os\n",
        "import shutil\n",
        "import glob\n",
        "import wfdb\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.models import load_model \n",
        "from tensorflow.keras.callbacks import TensorBoard\n",
        "import tensorflow_datasets as tfds\n",
        "import multiprocessing\n",
        "from datetime import datetime\n",
        "import sklearn.metrics\n",
        "import itertools\n",
        "import io\n",
        "import pickle\n",
        "print(tf.__version__)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The tensorboard extension is already loaded. To reload it, use:\n",
            "  %reload_ext tensorboard\n",
            "2.1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z8y478FMSLVU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#run this cell if multiple GPUs are used\n",
        "tf.debugging.set_log_device_placement(True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z5Qo9PRdSOdv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.python.client import device_lib \n",
        "print(device_lib.list_local_devices())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EBpW2rfESO-0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tf.test.is_built_with_cuda()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5_wxDhglSU7P",
        "colab_type": "text"
      },
      "source": [
        "#3.Data Pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zsPZhklkSiHI",
        "colab_type": "text"
      },
      "source": [
        "## 3.1 Input Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IQbRUHiBK1X6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#load signal\n",
        "with open('D:WFDB//matched/seg_PPG_signals_p09.pkl', \"rb\") as fp:\n",
        "  raw_signals = pickle.load(fp)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KKl8DpkGJiPk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "92b2b442-e45b-4bed-c12e-d50eb4a5dec9"
      },
      "source": [
        "#create the right dim\n",
        "print('signals dim:', raw_signals.shape)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "signals dim: (83197, 1, 3750, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W4sclRClSYcE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "6dfdadff-fe32-4dee-cffa-98d03274ad77"
      },
      "source": [
        "#load label\n",
        "df = pd.read_csv('D:WFDB//matched/ECG_Afib_labels_p09.csv',sep=',', header=None)\n",
        "labels = df.to_numpy()\n",
        "print('labels dim',labels.shape)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "labels dim (83197, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xprd3SoCSl71",
        "colab_type": "text"
      },
      "source": [
        "## 3.2 Extract, Transform and Load data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OXIJQPgJSz4Y",
        "colab_type": "text"
      },
      "source": [
        "###3.2.1 Parallelize Extraction (how?)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OA6BLiLnSQci",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "e5ea774e-3537-4fb2-edaa-bd2c5134a80a"
      },
      "source": [
        "dataset = tf.data.Dataset.from_tensor_slices((raw_signals,labels))\n",
        "dataset"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Executing op TensorSliceDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<TensorSliceDataset shapes: ((1, 3750, 1), (1,)), types: (tf.float64, tf.int64)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "07gKGNgmBxuR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "###ratio value is between 0 and 1\n",
        "def slice_dataset(dataset,train_ratio,seed = 1):   #make sure seed is set to a same number for repeatable results\n",
        "  DATASET_SIZE =len(list(dataset)) #only works in eager mode (e.g. TF version >= 2.0.x)\n",
        "  dataset = dataset.shuffle(DATASET_SIZE, seed)\n",
        "\n",
        "  train_size = int(train_ratio * DATASET_SIZE)\n",
        "  val_size = int((1-train_ratio) * DATASET_SIZE)\n",
        "\n",
        "  train_dataset = dataset.take(train_size)\n",
        "  val_dataset = dataset.skip(train_size)\n",
        "\n",
        "  return train_dataset,val_dataset"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bQ3w1C-GJk2O",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 496
        },
        "outputId": "b9b4fd41-4c73-4661-9734-b6629166ef95"
      },
      "source": [
        "#create training, validation and test dataset\n",
        "train_dataset, test = slice_dataset(dataset, 0.9)\n",
        "train, validation = slice_dataset(train_dataset,0.8)\n",
        "print('size of training data:',len(list(train)))\n",
        "print('size of validation data:',len(list(validation)))\n",
        "print('size of test data:',len(list(test)))"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Executing op OptimizeDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
            "Executing op ModelDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
            "Executing op AnonymousIteratorV2 in device /job:localhost/replica:0/task:0/device:CPU:0\n",
            "Executing op MakeIterator in device /job:localhost/replica:0/task:0/device:CPU:0\n",
            "Executing op IteratorGetNextSync in device /job:localhost/replica:0/task:0/device:CPU:0\n",
            "Executing op DeleteIterator in device /job:localhost/replica:0/task:0/device:CPU:0\n",
            "Executing op Equal in device /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Executing op LogicalAnd in device /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Executing op SelectV2 in device /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Executing op AnonymousRandomSeedGenerator in device /job:localhost/replica:0/task:0/device:CPU:0\n",
            "Executing op ShuffleDatasetV2 in device /job:localhost/replica:0/task:0/device:CPU:0\n",
            "Executing op TakeDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
            "Executing op SkipDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
            "Executing op OptimizeDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
            "Executing op ModelDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
            "Executing op ShuffleDatasetV2 in device /job:localhost/replica:0/task:0/device:CPU:0\n",
            "Executing op TakeDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
            "Executing op SkipDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
            "Executing op OptimizeDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
            "Executing op ModelDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
            "size of training data: 59901\n",
            "Executing op OptimizeDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
            "Executing op ModelDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
            "size of validation data: 14976\n",
            "Executing op OptimizeDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
            "Executing op ModelDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
            "size of test data: 8320\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0fResed9S2cp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "outputId": "88a7db78-c2a1-4761-f45e-cc1ae986dfee"
      },
      "source": [
        "#print and visualize the data\n",
        "for signal, label in tfds.as_numpy(dataset.take(2)):\n",
        "  print(signal.shape, label)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Executing op TakeDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
            "Executing op OptimizeDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
            "Executing op ModelDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
            "(1, 3750, 1) [0]\n",
            "(1, 3750, 1) [0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tn_mIRbwS6tQ",
        "colab_type": "text"
      },
      "source": [
        "### 3.2.2 Parallelize Transformation\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wf_TMonOS9Bo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cores = multiprocessing.cpu_count()\n",
        "print(cores)\n",
        "#dataset = dataset.map(function, num_parallel_calls = cores)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gq6jK1foTFCU",
        "colab_type": "text"
      },
      "source": [
        "### 3.2.3 Parallelize Loading"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uD2aSxbkTF96",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        },
        "outputId": "12ec6c90-5857-4f42-c270-7491ec6a2162"
      },
      "source": [
        "batch_size = 32\n",
        "train_dataset = train.cache()\n",
        "train_dataset = train_dataset.shuffle(1024).repeat().batch(batch_size,drop_remainder=True)\n",
        "train_dataset = train_dataset.prefetch(buffer_size = tf.data.experimental.AUTOTUNE)\n",
        "val_dataset = validation.repeat().batch(batch_size, drop_remainder=True)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Executing op CacheDatasetV2 in device /job:localhost/replica:0/task:0/device:CPU:0\n",
            "Executing op ShuffleDatasetV2 in device /job:localhost/replica:0/task:0/device:CPU:0\n",
            "Executing op RepeatDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
            "Executing op BatchDatasetV2 in device /job:localhost/replica:0/task:0/device:CPU:0\n",
            "Executing op PrefetchDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
            "Executing op RepeatDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
            "Executing op BatchDatasetV2 in device /job:localhost/replica:0/task:0/device:CPU:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ikiF9-uDTI-h",
        "colab_type": "text"
      },
      "source": [
        "#4. Train Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MeF1mUEJTN60",
        "colab_type": "text"
      },
      "source": [
        "##4.1 Build the model and find the optimal learning rate"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zwPKkjtNlpUK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#create a model\n",
        "model = tf.keras.Sequential([\n",
        "    #1st Conv2D\n",
        "    tf.keras.layers.Conv2D(8, (1, 1), strides=(1, 1), \n",
        "                          activation='relu', input_shape=(1,3750,1)),\n",
        "    tf.keras.layers.BatchNormalization(),\n",
        "    tf.keras.layers.MaxPooling2D(pool_size=(1, 2),strides=(1, 2)),\n",
        "    tf.keras.layers.Dropout(0.2),\n",
        "    #2nd Conv2D\n",
        "    tf.keras.layers.Conv2D(16, (1, 3), strides=(1, 1),\n",
        "                          activation='relu'),\n",
        "    tf.keras.layers.BatchNormalization(),\n",
        "    tf.keras.layers.MaxPooling2D(pool_size=(1, 2),strides=(1, 2)),\n",
        "    tf.keras.layers.Dropout(0.2),\n",
        "    #3rd Conv2D\n",
        "    tf.keras.layers.Conv2D(32, (1, 3), strides=(1, 1),\n",
        "                          activation='relu'),\n",
        "    tf.keras.layers.BatchNormalization(),\n",
        "    tf.keras.layers.MaxPooling2D(pool_size=(1, 2),strides=(1, 2)),\n",
        "    tf.keras.layers.Dropout(0.2),\n",
        "    #4th Conv2D\n",
        "    tf.keras.layers.Conv2D(64, (1, 3), strides=(1, 1),\n",
        "                          activation='relu'),\n",
        "    tf.keras.layers.BatchNormalization(),\n",
        "    tf.keras.layers.MaxPooling2D(pool_size=(1, 2),strides=(1, 2)),\n",
        "    tf.keras.layers.Dropout(0.2),\n",
        "    #5th Conv2D\n",
        "    tf.keras.layers.Conv2D(16, (1, 1), strides=(1, 1),\n",
        "                          activation='relu'),\n",
        "    tf.keras.layers.BatchNormalization(),\n",
        "    #Full connection layer\n",
        "    tf.keras.layers.Flatten(),\n",
        "    #tf.keras.layers.LSTM(50, stateful=True, return_sequences=True),\n",
        "    #tf.keras.layers.LSTM(10, stateful=True),\n",
        "    tf.keras.layers.Dense(64, activation='relu'),\n",
        "    tf.keras.layers.BatchNormalization(),\n",
        "    tf.keras.layers.Dropout(0.2),\n",
        "    tf.keras.layers.Dense(1, activation = 'sigmoid')\n",
        "])\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qSGbK_IbTLMF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "f41d40ed-b123-458a-cbbe-1d246e19c5ca"
      },
      "source": [
        "#clear history if necessary\n",
        "tf.keras.backend.clear_session()\n",
        "\n",
        "#train model\n",
        "learning_rate = 0.0001\n",
        "strategy = tf.distribute.MirroredStrategy(cross_device_ops=tf.distribute.HierarchicalCopyAllReduce()) ##to overwrite NCCL cross device communication as this is running in Windows\n",
        "with strategy.scope():\n",
        "\n",
        "  model = model\n",
        "\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(lr=learning_rate), \n",
        "              loss=tf.keras.losses.binary_crossentropy, \n",
        "              metrics=['accuracy'])\n",
        "\n",
        "#callback: schedule a learning rate incline iteration\n",
        "lr_schedule = tf.keras.callbacks.LearningRateScheduler(\n",
        "    lambda epoch: 1e-4 * 10**(epoch / 20))\n",
        "\n",
        "#start training\n",
        "history = model.fit(train_dataset,\n",
        "                    epochs=10,\n",
        "                    steps_per_epoch = len(list(train))/batch_size,\n",
        "                    verbose=1,\n",
        "                    validation_data=val_dataset,\n",
        "                    validation_steps = len(list(validation))/batch_size,\n",
        "                    callbacks=[lr_schedule]\n",
        "                    )"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1', '/job:localhost/replica:0/task:0/device:GPU:2', '/job:localhost/replica:0/task:0/device:GPU:3')\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1', '/job:localhost/replica:0/task:0/device:GPU:2', '/job:localhost/replica:0/task:0/device:GPU:3')\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d (Conv2D)              (None, 1, 3750, 8)        16        \n",
            "_________________________________________________________________\n",
            "batch_normalization (BatchNo (None, 1, 3750, 8)        32        \n",
            "_________________________________________________________________\n",
            "max_pooling2d (MaxPooling2D) (None, 1, 1875, 8)        0         \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 1, 1875, 8)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_1 (Conv2D)            (None, 1, 1873, 16)       400       \n",
            "_________________________________________________________________\n",
            "batch_normalization_1 (Batch (None, 1, 1873, 16)       64        \n",
            "_________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2 (None, 1, 936, 16)        0         \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 1, 936, 16)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 1, 934, 32)        1568      \n",
            "_________________________________________________________________\n",
            "batch_normalization_2 (Batch (None, 1, 934, 32)        128       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2 (None, 1, 467, 32)        0         \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 1, 467, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_3 (Conv2D)            (None, 1, 465, 64)        6208      \n",
            "_________________________________________________________________\n",
            "batch_normalization_3 (Batch (None, 1, 465, 64)        256       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_3 (MaxPooling2 (None, 1, 232, 64)        0         \n",
            "_________________________________________________________________\n",
            "dropout_3 (Dropout)          (None, 1, 232, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_4 (Conv2D)            (None, 1, 232, 16)        1040      \n",
            "_________________________________________________________________\n",
            "batch_normalization_4 (Batch (None, 1, 232, 16)        64        \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 3712)              0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 64)                237632    \n",
            "_________________________________________________________________\n",
            "batch_normalization_5 (Batch (None, 64)                256       \n",
            "_________________________________________________________________\n",
            "dropout_4 (Dropout)          (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 1)                 65        \n",
            "=================================================================\n",
            "Total params: 247,729\n",
            "Trainable params: 247,329\n",
            "Non-trainable params: 400\n",
            "_________________________________________________________________\n",
            "Executing op OptimizeDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
            "Executing op ModelDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JXVnKYMLTQ4j",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 291
        },
        "outputId": "5fed640b-f51e-479f-f78a-b410a1b66010"
      },
      "source": [
        "#Visualize learning rate vs epoches\n",
        "plt.semilogx(history.history[\"lr\"], history.history[\"loss\"])\n",
        "plt.axis([1e-4, 1e-2,0,1])"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.0001, 0.01, 0, 1]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t2ril_S4TVJS",
        "colab_type": "text"
      },
      "source": [
        "##4.2 Train the model with the optimal learning rate"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4L2FJoWOTWJ8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def plot_to_image(figure):\n",
        "    \"\"\"\n",
        "    Converts the matplotlib plot specified by 'figure' to a PNG image and\n",
        "    returns it. The supplied figure is closed and inaccessible after this call.\n",
        "    \"\"\"\n",
        "    \n",
        "    buf = io.BytesIO()\n",
        "    \n",
        "    # EXERCISE: Use plt.savefig to save the plot to a PNG in memory.\n",
        "    # YOUR CODE HERE\n",
        "    plt.savefig(buf, format='png')\n",
        "    # Closing the figure prevents it from being displayed directly inside\n",
        "    # the notebook.\n",
        "    plt.close(figure)\n",
        "    buf.seek(0)\n",
        "    \n",
        "    # EXERCISE: Use tf.image.decode_png to convert the PNG buffer\n",
        "    # to a TF image. Make sure you use 4 channels.\n",
        "    image = tf.image.decode_png(buf.getvalue(), channels=4)\n",
        "    \n",
        "    # EXERCISE: Use tf.expand_dims to add the batch dimension\n",
        "    image = tf.expand_dims(image,0)\n",
        "    \n",
        "    return image"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P81b0Rp0TX81",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#https://axbihaqixpqbrrxincyxja.coursera-apps.org/notebooks/week3/TF_Serving_Week_3_Exercise_Question.ipynb\n",
        "\n",
        "class_names = ['NO Afib','Afib']\n",
        "\n",
        "def plot_confusion_matrix(cm, class_names, normalize=False):\n",
        "    \"\"\"\n",
        "    Returns a matplotlib figure containing the plotted confusion matrix.\n",
        "    \n",
        "    Args:\n",
        "       cm (array, shape = [n, n]): a confusion matrix of integer classes\n",
        "       class_names (array, shape = [n]): String names of the integer classes\n",
        "    \"\"\"\n",
        "    \n",
        "    figure = plt.figure(figsize=(8, 8))\n",
        "    plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
        "    plt.title(\"Confusion matrix\")\n",
        "    plt.colorbar()\n",
        "    tick_marks = np.arange(len(class_names))\n",
        "    plt.xticks(tick_marks, class_names)\n",
        "    plt.yticks(tick_marks, class_names)\n",
        "    plt.ylim(bottom=1.5,top = -0.5)\n",
        "    \n",
        "    if normalize:\n",
        "      cm = np.around(cm.astype('float') / cm.sum(axis=1)[:, np.newaxis], decimals=2)\n",
        "    \n",
        "    # Use white text if squares are dark; otherwise black.\n",
        "    threshold = cm.max() / 1.5\n",
        "    \n",
        "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
        "      plt.text(j, i, cm[i, j], \n",
        "               horizontalalignment=\"center\", \n",
        "               verticalalignment='center', \n",
        "               color=\"white\" if cm[i, j] > threshold else \"black\")\n",
        "        \n",
        "    plt.tight_layout()\n",
        "    plt.ylabel('True label')\n",
        "    plt.xlabel('Predicted label')\n",
        "    return figure"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9hAk0QPETZ95",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def log_confusion_matrix(epoch, logs):\n",
        "    \n",
        "    # EXERCISE: Use the model to predict the values from the test_images.\n",
        "    test_pred_raw = model.predict(test)\n",
        "    \n",
        "    test_pred = np.where(test_pred_raw > 0.5, 1, 0)\n",
        "    \n",
        "    # EXERCISE: Calculate the confusion matrix using sklearn.metrics\n",
        "    cm = sklearn.metrics.confusion_matrix(test_labels, test_pred)\n",
        "    \n",
        "    figure = plot_confusion_matrix(cm, class_names=class_names, normalize = True)\n",
        "    cm_image = plot_to_image(figure)\n",
        "    \n",
        "    # Log the confusion matrix as an image summary.\n",
        "    with file_writer_cm.as_default():\n",
        "        tf.summary.image(\"Confusion Matrix\", cm_image, step=epoch)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "boYor7DsTcXT",
        "colab_type": "text"
      },
      "source": [
        "### 4.2.2 Start Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-eqeVT1uTeOn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "d8df0124-a819-4a60-fe88-fa2be37f167f"
      },
      "source": [
        "%%capture\n",
        "#clear history if necessary\n",
        "tf.keras.backend.clear_session()\n",
        "\n",
        "#train model\n",
        "learning_rate = 0.003 #choose the optimal learning rate\n",
        "strategy = tf.distribute.MirroredStrategy(cross_device_ops=tf.distribute.HierarchicalCopyAllReduce()) ##to overwrite NCCL cross device communication as this is running in Windows\n",
        "with strategy.scope():\n",
        "\n",
        "  model = model\n",
        "\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(lr=learning_rate), \n",
        "              loss=tf.keras.losses.binary_crossentropy, \n",
        "              metrics=['accuracy'])\n",
        "\n",
        "#callback: tensorboard\n",
        "log_dir=\"logs\\\\fit\\\\\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "file_writer_cm = tf.summary.create_file_writer(log_dir + '/cm')\n",
        "\n",
        "cm_callback = keras.callbacks.LambdaCallback(on_epoch_end=log_confusion_matrix)\n",
        "tensorboard_callback = TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
        "\n",
        "#start training\n",
        "model.fit(train_dataset,\n",
        "          epochs=100,\n",
        "          steps_per_epoch = len(list(train))/batch_size,\n",
        "          verbose=0,\n",
        "          validation_data=val_dataset,\n",
        "          validation_steps = len(list(validation))/batch_size,\n",
        "          callbacks=[tensorboard_callback, cm_callback]\n",
        "          )"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1', '/job:localhost/replica:0/task:0/device:GPU:2', '/job:localhost/replica:0/task:0/device:GPU:3')\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1', '/job:localhost/replica:0/task:0/device:GPU:2', '/job:localhost/replica:0/task:0/device:GPU:3')\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:batch_all_reduce: 26 all-reduces with algorithm = hierarchical_copy, num_packs = 1, agg_small_grads_max_bytes = 0 and agg_small_grads_max_group = 10\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:batch_all_reduce: 26 all-reduces with algorithm = hierarchical_copy, num_packs = 1, agg_small_grads_max_bytes = 0 and agg_small_grads_max_group = 10\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:batch_all_reduce: 26 all-reduces with algorithm = hierarchical_copy, num_packs = 1, agg_small_grads_max_bytes = 0 and agg_small_grads_max_group = 10\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:batch_all_reduce: 26 all-reduces with algorithm = hierarchical_copy, num_packs = 1, agg_small_grads_max_bytes = 0 and agg_small_grads_max_group = 10\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[1;32m<ipython-input-49-119d7a0efe93>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     64\u001b[0m           \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mval_dataset\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m           \u001b[0mvalidation_steps\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalidation\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;36m0.2\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 66\u001b[1;33m           \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtensorboard_callback\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcm_callback\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     67\u001b[0m           )\n",
            "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    817\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    818\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 819\u001b[1;33m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[0;32m    820\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    821\u001b[0m   def evaluate(self,\n",
            "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    395\u001b[0m                       total_epochs=1)\n\u001b[0;32m    396\u001b[0m                   cbks.make_logs(model, epoch_logs, eval_result, ModeKeys.TEST,\n\u001b[1;32m--> 397\u001b[1;33m                                  prefix='val_')\n\u001b[0m\u001b[0;32m    398\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    399\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\Anaconda3\\lib\\contextlib.py\u001b[0m in \u001b[0;36m__exit__\u001b[1;34m(self, type, value, traceback)\u001b[0m\n\u001b[0;32m    117\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mtype\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    118\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 119\u001b[1;33m                 \u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    120\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    121\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36mon_epoch\u001b[1;34m(self, epoch, mode)\u001b[0m\n\u001b[0;32m    769\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mmode\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mModeKeys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    770\u001b[0m         \u001b[1;31m# Epochs only apply to `fit`.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 771\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    772\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprogbar\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    773\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\callbacks.py\u001b[0m in \u001b[0;36mon_epoch_end\u001b[1;34m(self, epoch, logs)\u001b[0m\n\u001b[0;32m    300\u001b[0m     \u001b[0mlogs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlogs\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    301\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mcallback\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 302\u001b[1;33m       \u001b[0mcallback\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    303\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    304\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m<ipython-input-41-f4f2ea0c09ec>\u001b[0m in \u001b[0;36mlog_confusion_matrix\u001b[1;34m(epoch, logs)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[1;31m# EXERCISE: Use the model to predict the values from the test_images.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[0mtest_pred_raw\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0mtest_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_pred_raw\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0.5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1011\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1012\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1013\u001b[1;33m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[0;32m   1014\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1015\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mreset_metrics\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, model, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    496\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mModeKeys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPREDICT\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    497\u001b[0m         \u001b[0msteps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_queue_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 498\u001b[1;33m         workers=workers, use_multiprocessing=use_multiprocessing, **kwargs)\n\u001b[0m\u001b[0;32m    499\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    500\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36m_model_iteration\u001b[1;34m(self, model, mode, x, y, batch_size, verbose, sample_weight, steps, callbacks, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    424\u001b[0m           \u001b[0mmax_queue_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    425\u001b[0m           \u001b[0mworkers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 426\u001b[1;33m           use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[0;32m    427\u001b[0m       \u001b[0mtotal_samples\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_get_total_number_of_samples\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0madapter\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    428\u001b[0m       \u001b[0muse_sample\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtotal_samples\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36m_process_inputs\u001b[1;34m(model, mode, x, y, batch_size, epochs, sample_weights, class_weights, shuffle, steps, distribution_strategy, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m    704\u001b[0m       \u001b[0mmax_queue_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    705\u001b[0m       \u001b[0mworkers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 706\u001b[1;33m       use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[0;32m    707\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    708\u001b[0m   \u001b[1;32mreturn\u001b[0m \u001b[0madapter\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, x, y, sample_weights, standardize_function, **kwargs)\u001b[0m\n\u001b[0;32m    700\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    701\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mstandardize_function\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 702\u001b[1;33m       \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstandardize_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    703\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    704\u001b[0m     \u001b[1;31m# Note that the dataset instance is immutable, its fine to reusing the user\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36mstandardize_function\u001b[1;34m(dataset)\u001b[0m\n\u001b[0;32m    682\u001b[0m           \u001b[1;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    683\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 684\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmap_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_parallel_calls\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdataset_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mAUTOTUNE\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    685\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    686\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mmode\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mModeKeys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPREDICT\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\data\\ops\\dataset_ops.py\u001b[0m in \u001b[0;36mmap\u001b[1;34m(self, map_func, num_parallel_calls)\u001b[0m\n\u001b[0;32m   1589\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1590\u001b[0m       return ParallelMapDataset(\n\u001b[1;32m-> 1591\u001b[1;33m           self, map_func, num_parallel_calls, preserve_cardinality=True)\n\u001b[0m\u001b[0;32m   1592\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1593\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mflat_map\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmap_func\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\data\\ops\\dataset_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, input_dataset, map_func, num_parallel_calls, use_inter_op_parallelism, preserve_cardinality, use_legacy_function)\u001b[0m\n\u001b[0;32m   3924\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_transformation_name\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3925\u001b[0m         \u001b[0mdataset\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minput_dataset\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3926\u001b[1;33m         use_legacy_function=use_legacy_function)\n\u001b[0m\u001b[0;32m   3927\u001b[0m     self._num_parallel_calls = ops.convert_to_tensor(\n\u001b[0;32m   3928\u001b[0m         num_parallel_calls, dtype=dtypes.int32, name=\"num_parallel_calls\")\n",
            "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\data\\ops\\dataset_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, func, transformation_name, dataset, input_classes, input_shapes, input_types, input_structure, add_to_graph, use_legacy_function, defun_kwargs)\u001b[0m\n\u001b[0;32m   3145\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mtracking\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresource_tracker_scope\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresource_tracker\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3146\u001b[0m         \u001b[1;31m# TODO(b/141462134): Switch to using garbage collection.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3147\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mwrapper_fn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_concrete_function_internal\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3148\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3149\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0madd_to_graph\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_get_concrete_function_internal\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2393\u001b[0m     \u001b[1;34m\"\"\"Bypasses error checking when getting a graph function.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2394\u001b[0m     graph_function = self._get_concrete_function_internal_garbage_collected(\n\u001b[1;32m-> 2395\u001b[1;33m         *args, **kwargs)\n\u001b[0m\u001b[0;32m   2396\u001b[0m     \u001b[1;31m# We're returning this concrete function to someone, and they may keep a\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2397\u001b[0m     \u001b[1;31m# reference to the FuncGraph without keeping a reference to the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_get_concrete_function_internal_garbage_collected\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2387\u001b[0m       \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2388\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2389\u001b[1;33m       \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2390\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2391\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[1;34m(self, args, kwargs)\u001b[0m\n\u001b[0;32m   2701\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2702\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmissed\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcall_context_key\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2703\u001b[1;33m       \u001b[0mgraph_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_create_graph_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2704\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprimary\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcache_key\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2705\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[1;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[0;32m   2591\u001b[0m             \u001b[0marg_names\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0marg_names\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2592\u001b[0m             \u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2593\u001b[1;33m             capture_by_value=self._capture_by_value),\n\u001b[0m\u001b[0;32m   2594\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_function_attributes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2595\u001b[0m         \u001b[1;31m# Tell the ConcreteFunction to clean up its graph once it goes out of\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[1;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\u001b[0m\n\u001b[0;32m    976\u001b[0m                                           converted_func)\n\u001b[0;32m    977\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 978\u001b[1;33m       \u001b[0mfunc_outputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    979\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    980\u001b[0m       \u001b[1;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\data\\ops\\dataset_ops.py\u001b[0m in \u001b[0;36mwrapper_fn\u001b[1;34m(*args)\u001b[0m\n\u001b[0;32m   3138\u001b[0m           attributes=defun_kwargs)\n\u001b[0;32m   3139\u001b[0m       \u001b[1;32mdef\u001b[0m \u001b[0mwrapper_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=missing-docstring\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3140\u001b[1;33m         \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_wrapper_helper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3141\u001b[0m         \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstructure\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_tensor_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_output_structure\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mret\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3142\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mret\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\data\\ops\\dataset_ops.py\u001b[0m in \u001b[0;36m_wrapper_helper\u001b[1;34m(*args)\u001b[0m\n\u001b[0;32m   3080\u001b[0m         \u001b[0mnested_args\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mnested_args\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3081\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3082\u001b[1;33m       \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mautograph\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtf_convert\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mag_ctx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mnested_args\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3083\u001b[0m       \u001b[1;31m# If `func` returns a list of tensors, `nest.flatten()` and\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3084\u001b[0m       \u001b[1;31m# `ops.convert_to_tensor()` would conspire to attempt to stack\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\autograph\\impl\\api.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    235\u001b[0m       \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint:disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    236\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'ag_error_metadata'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 237\u001b[1;33m           \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    238\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    239\u001b[0m           \u001b[1;32mraise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;31mValueError\u001b[0m: in converted code:\n\n    C:\\Users\\57lzhang.US04WW4008\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py:677 map_fn\n        batch_size=None)\n    C:\\Users\\57lzhang.US04WW4008\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py:2410 _standardize_tensors\n        exception_prefix='input')\n    C:\\Users\\57lzhang.US04WW4008\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_utils.py:573 standardize_input_data\n        'with shape ' + str(data_shape))\n\n    ValueError: Error when checking input: expected conv2d_input to have 4 dimensions, but got array with shape (1, 3750, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7f4i7w_DThVs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 838
        },
        "outputId": "6f9307ef-647f-4fec-b23b-6f4dc6a6d49d"
      },
      "source": [
        "tensorboard --logdir=log_dir"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Reusing TensorBoard on port 6006 (pid 3240), started 0:03:25 ago. (Use '!kill 3240' to kill it.)"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "      <iframe id=\"tensorboard-frame-fcd135f97f6e13a1\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
              "      </iframe>\n",
              "      <script>\n",
              "        (function() {\n",
              "          const frame = document.getElementById(\"tensorboard-frame-fcd135f97f6e13a1\");\n",
              "          const url = new URL(\"/\", window.location);\n",
              "          url.port = 6006;\n",
              "          frame.src = url;\n",
              "        })();\n",
              "      </script>\n",
              "  "
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fbUv7BhOTiua",
        "colab_type": "text"
      },
      "source": [
        "## 4.3 Save Model for future evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fw29XiWcTlxh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install -q pyyaml h5py"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ricCF8i5Tnoz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "os.chdir(r\"C:\\Users\\57lzhang.US04WW4008\\Desktop\\Afib\\Afib_ECG data\")\n",
        "model.save('Deep_ECG.h5')\n",
        "print(\"Save model to disk\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IfnasVq2Tqjg",
        "colab_type": "text"
      },
      "source": [
        "# 5. Model Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0a8QzHAVT1bz",
        "colab_type": "text"
      },
      "source": [
        "## 5.1 Load saved model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nZyzAHekTsy5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "os.chdir(r\"C:\\Users\\57lzhang.US04WW4008\\Desktop\\Afib\\Afib_ECG data\")\n",
        "model = tf.keras.models.load_model('Deep_ECG.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6aMp1YpLTw7C",
        "colab_type": "text"
      },
      "source": [
        "## 5.2 Confusion Matrix"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nJD3CxiTTuse",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "threshold = 0.2\n",
        "test_pred_raw = model.predict(test_signals)\n",
        "test_pred = np.where(test_pred_raw > threshold, 1, 0)\n",
        "# EXERCISE: Calculate the confusion matrix using sklearn.metrics\n",
        "cm = sklearn.metrics.confusion_matrix(test_labels, test_pred)\n",
        "figure_norm = plot_confusion_matrix(cm, class_names=class_names, normalize=True)\n",
        "figure_norm.show()\n",
        "figure = plot_confusion_matrix(cm, class_names=class_names, normalize=False)\n",
        "figure.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Yq00AMzT5n6",
        "colab_type": "text"
      },
      "source": [
        "## 5.3 F-1 Score"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e6_xv9F1T7xx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "report = sklearn.metrics.classification_report(test_labels, test_pred)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZnnZD1InT9_R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(report)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P6DNehETT_dL",
        "colab_type": "text"
      },
      "source": [
        "## 5.4 AUC"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5cextNotUBHg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "score = sklearn.metrics.roc_auc_score(test_labels, test_pred)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7cNSvIDgUC9D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(score)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YSc3QXS2UEJv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import sklearn.metrics as metrics\n",
        "probs = model.predict_proba(test_signals)\n",
        "preds = probs[:,]\n",
        "fpr, tpr, threshold = metrics.roc_curve(test_labels, preds)\n",
        "roc_auc = metrics.auc(fpr, tpr)\n",
        "\n",
        "plt.title('Receiver Operating Characteristic')\n",
        "plt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\n",
        "plt.legend(loc = 'lower right')\n",
        "plt.plot([0, 1], [0, 1],'r--')\n",
        "plt.xlim([0, 1])\n",
        "plt.ylim([0, 1])\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}