{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Afib_ECG.ipynb",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/supertime1/Afib_PPG/blob/master/Afib_ECG.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-oNUq1dRJq-r",
        "colab_type": "text"
      },
      "source": [
        "#1.Introduction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DIS5QlL3G0G7",
        "colab_type": "text"
      },
      "source": [
        "This notebook trains an ECG DNN by using labeled ECG data from \"The PhysioNet Computing in Cardiology Challenge 2017\" (https://physionet.org/content/challenge-2017/1.0.0/). The ECG DNN model will be used to label the ECG data from MIMIC-III waveform dataset, so as the concurrent PPG data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jSibYKc0KaNC",
        "colab_type": "text"
      },
      "source": [
        "The ECG data used in training and validation has the following important attributes:\n",
        "*   sampling frequency: 300Hz\n",
        "*   4 lables: Normal (N), AF (A), Other rhythm (O), Noisy (~)\n",
        "*   length: 9 - 60s with 30s mean.\n",
        "*   preprosessed: data has been band pass filtered by AliveCor device\n",
        "\n",
        "\n",
        "Only time length >30s is used in training, since PPG data usually requires 30s for Afib detection."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w9YY9bo0CLNy",
        "colab_type": "text"
      },
      "source": [
        "#2.Setup Environment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xa66CINNKj1y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install tensorflow-gpu"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q95xF5tmemik",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install wfdb"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GJnlylcLwF5x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install tensorflow_datasets"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NGo63wdRGY03",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from IPython.display import display\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "%load_ext tensorboard\n",
        "import numpy as np\n",
        "import os\n",
        "import shutil\n",
        "import glob\n",
        "import wfdb\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.models import load_model \n",
        "from tensorflow.keras.callbacks import TensorBoard\n",
        "import tensorflow_datasets as tfds\n",
        "import multiprocessing\n",
        "from datetime import datetime\n",
        "import sklearn.metrics\n",
        "import itertools\n",
        "import io\n",
        "print(tf.__version__)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UsKVuNWV6t4v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#run this cell if multiple GPUs are used\n",
        "tf.debugging.set_log_device_placement(True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xia3EgQ6-gNN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.python.client import device_lib \n",
        "print(device_lib.list_local_devices())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FuQ_w0-N-nrc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tf.test.is_built_with_cuda()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZyD6ZPEDKyYw",
        "colab_type": "text"
      },
      "source": [
        "#3.Data Pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fkIhig4MCdvc",
        "colab_type": "text"
      },
      "source": [
        "##3.1 Load data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3C6pSdgSIGvl",
        "colab_type": "text"
      },
      "source": [
        "###3.1.1 Load training data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Totv4fFHKwED",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "hd_names = []\n",
        "for name in glob.glob(\"C:/Users/57lzhang.US04WW4008/Desktop/Afib/Afib_ECG data/training2017/*.hea\"):#'/content/drive/My Drive/training2017/*.hea'): \n",
        "  position = name.index('.hea')\n",
        "  name = name[0:position] #remove the .hea part to comply with the wfdb.rdrecord format\n",
        "  hd_names.append(name)\n",
        "print('There are total', len(hd_names), 'records')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "50P04jUzLgZG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "qualified_names = [] #a list of file names that contain ECG Lead I data\n",
        "for name in hd_names:\n",
        "  record = wfdb.rdheader(name)\n",
        "  if record.sig_len >= 9000: #extact only records contrains ECG lead I >30s\n",
        "    qualified_names.append(name)\n",
        "print('There are total', len(qualified_names), 'qualified (>30s) records')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "39lu0EfVOxso",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#load label numpy file\n",
        "df = pd.read_csv(r'C:\\Users\\57lzhang.US04WW4008\\Desktop\\Afib\\Afib_ECG data\\training2017\\REFERENCE.csv', sep=',', header=None) #'/content/drive/My Drive/training2017/REFERENCE.csv',sep=',', header=None)#\n",
        "#create a new name list that only stores the file name\n",
        "init_labels =[]\n",
        "new_names = []\n",
        "for name in qualified_names:\n",
        "  temp_name = name[-6:] #remove the dir and keep only the file name\n",
        "  temp_label = df[df[0] == temp_name][1].to_numpy()\n",
        "  if temp_label != '~':\n",
        "    init_labels.append(temp_label)\n",
        "    new_names.append(name)\n",
        "init_labels = np.array(init_labels)\n",
        "\n",
        "init_labels[init_labels =='N'] = '0'\n",
        "init_labels[init_labels =='A'] = '1'\n",
        "init_labels[init_labels =='O'] = '0'\n",
        "\n",
        "print('There are total', len(init_labels),'none-nosiy labels')\n",
        "print('There are total', len(new_names), 'none-nosiy records')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ribTthakZcx3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "##read signals\n",
        "ECG_signals = [] #create a  list to store all  ECG signals\n",
        "for name in new_names:\n",
        "  record = wfdb.rdrecord(name)\n",
        "  ECG_signals.append(record.p_signal)\n",
        "\n",
        "print('ECG signals len:', len(ECG_signals))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "koI2rxjvaCAv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#A function to split the raw signal data with 30s per segment and keep the label\n",
        "##source is the raw signal (e.g. ECG_signals) and seg_len = 30s * 300Hz = 9000\n",
        "def generate_segment_data(source,init_labels,seg_len):\n",
        "  n=0\n",
        "  signals =[]\n",
        "  labels = []\n",
        "  for signal in source:\n",
        "    for i in range(int(len(signal)/seg_len)):\n",
        "      seg = signal[seg_len*i:seg_len*(i+1)]\n",
        "      label = init_labels[n]\n",
        "      signals.append(seg)\n",
        "      labels.append(label)\n",
        "    n+=1\n",
        "#convert list into a numpy array and change its dim from (num of records, seg_len, 1) to (num of records, seg_len)\n",
        "  signals = np.asarray(list(map(lambda x: np.reshape(x,9000),signals)))\n",
        "  labels = np.asarray(list(map(lambda x: np.reshape(x,1),labels)))\n",
        "\n",
        "  return signals,labels"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zVL-5Pxdc7C5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#use generate_segment_data() to generate segments with labels\n",
        "#After segmentation, more data than previous is generated, because some source data are 60s \n",
        "signals, labels = generate_segment_data(ECG_signals,init_labels,9000)\n",
        "print('signals dim:',signals.shape)\n",
        "print('labels dim:',labels.shape)\n",
        "#resize the dimension for use in CNN\n",
        "signals = np.expand_dims(signals,axis=1)\n",
        "signals = np.expand_dims(signals,axis=3)\n",
        "print('signals dim after resize:',signals.shape)\n",
        "print('labels dim after resize:',labels.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "vrc6uk1yZEjr",
        "colab": {}
      },
      "source": [
        "#check unique labels\n",
        "unique, count = np.unique(labels,return_counts=True)\n",
        "print('There are', count[0], 'No Afib records in training dataset')\n",
        "print('There are', count[1], 'Afib records training dataset')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2X0nuo2WIN5z",
        "colab_type": "text"
      },
      "source": [
        "###3.1.2 Load test data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SOKk0RBjFdYp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#load test data\n",
        "test_hd_names = []\n",
        "for name in glob.glob(\"C:/Users/57lzhang.US04WW4008/Desktop/Afib/Afib_ECG data/validation/*.hea\"):#'/content/drive/My Drive/training2017/*.hea'): \n",
        "  position = name.index('.hea')\n",
        "  name = name[0:position] #remove the .hea part to comply with the wfdb.rdrecord format\n",
        "  test_hd_names.append(name)\n",
        "print('There are total', len(test_hd_names), 'test records')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i4RhLECVIfEC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_qualified_names = [] #a list of file names that contain ECG Lead I data\n",
        "for name in test_hd_names:\n",
        "  record = wfdb.rdheader(name)\n",
        "  if record.sig_len >= 9000: #extact only records contrains ECG lead I >30s\n",
        "    test_qualified_names.append(name)\n",
        "print('There are total', len(test_qualified_names), 'qualified (>30s) test records')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2n8cDwNIIk7g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#load label numpy file\n",
        "df = pd.read_csv(r'C:\\Users\\57lzhang.US04WW4008\\Desktop\\Afib\\Afib_ECG data\\validation\\REFERENCE.csv', sep=',', header=None) #'/content/drive/My Drive/training2017/REFERENCE.csv',sep=',', header=None)#\n",
        "#create a new name list that only stores the file name\n",
        "test_init_labels =[]\n",
        "test_new_names = []\n",
        "for name in test_qualified_names:\n",
        "  temp_name = name[-6:] #remove the dir and keep only the file name\n",
        "  temp_label = df[df[0] == temp_name][1].to_numpy()\n",
        "  if temp_label != '~':\n",
        "    test_init_labels.append(temp_label)\n",
        "    test_new_names.append(name)\n",
        "test_init_labels = np.array(test_init_labels)\n",
        "\n",
        "test_init_labels[test_init_labels =='N'] = '0'\n",
        "test_init_labels[test_init_labels =='A'] = '1'\n",
        "test_init_labels[test_init_labels =='O'] = '0'\n",
        "\n",
        "print('There are total', len(test_init_labels),'none-nosiy test labels')\n",
        "print('There are total', len(test_new_names), 'none-nosiy test records')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KQ1dxW5DImG8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "##read signals\n",
        "test_ECG_signals = [] #create a  list to store all  ECG signals\n",
        "for name in test_new_names:\n",
        "  record = wfdb.rdrecord(name)\n",
        "  test_ECG_signals.append(record.p_signal)\n",
        "\n",
        "print('test ECG signals len:', len(test_ECG_signals))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PaxdW87UI1wF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#use generate_segment_data() to generate segments with labels\n",
        "#After segmentation, more data than previous is generated, because some source data are 60s \n",
        "test_signals, test_labels = generate_segment_data(test_ECG_signals,test_init_labels,9000)\n",
        "print('test signals dim:',test_signals.shape)\n",
        "print('test labels dim:',test_labels.shape)\n",
        "#resize the dimension for use in CNN\n",
        "test_signals = np.expand_dims(test_signals,axis=1)\n",
        "test_signals = np.expand_dims(test_signals,axis=3)\n",
        "print('test signals dim after resize:',test_signals.shape)\n",
        "print('test labels dim after resize:',test_labels.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "368K5eYZR6gX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#check unique labels\n",
        "unique, count = np.unique(test_labels,return_counts=True)\n",
        "print('There are', count[0], 'No Afib records in test dataset')\n",
        "print('There are', count[1], 'Afib records test dataset')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZsSsDMDSjW0V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#convert test labels to floating point type, so that it can be compared with model output\n",
        "test_labels = test_labels.flatten()\n",
        "test_labels = test_labels.astype(float)\n",
        "type(test_labels[0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MkBe67QpCuEf",
        "colab_type": "text"
      },
      "source": [
        "##3.2 Extract, Transform and Load data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RBWlyLWskq8C",
        "colab_type": "text"
      },
      "source": [
        "###3.2.1 Parallelize Extraction (how?)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yb-w63Qfbo6u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "labels = tf.strings.to_number(labels) \n",
        "dataset = tf.data.Dataset.from_tensor_slices((signals,labels))\n",
        "dataset"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NlxfJjfzffG3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#print and visualize the data\n",
        "for signal, label in tfds.as_numpy(dataset.take(2)):\n",
        "  print(signal.shape, label)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TO8KaCDab4Yf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#optional: data preprocessing use dataset.map(function)\n",
        "#possible functions:\n",
        "#1.Spectrogram\n",
        "#2.Hample filter\n",
        "#3.Flat line and peak removal\n",
        "#4.Normalization"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J5wqFGtUkFwj",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "### 3.2.2 Parallelize Transformation\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "82EjsJdzjZF8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cores = multiprocessing.cpu_count()\n",
        "print(cores)\n",
        "#dataset = dataset.map(function, num_parallel_calls = cores)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_WPHNzkD-baa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "###ratio value is between 0 and 1\n",
        "def slice_dataset(dataset,train_ratio):\n",
        "  DATASET_SIZE =len(list(dataset)) #only works in eager mode (e.g. TF version >= 2.0.x)\n",
        "  dataset = dataset.shuffle(DATASET_SIZE)\n",
        "\n",
        "  train_size = int(train_ratio * DATASET_SIZE)\n",
        "  val_size = int((1-train_ratio) * DATASET_SIZE)\n",
        "\n",
        "  train_dataset = dataset.take(train_size)\n",
        "  val_dataset = dataset.skip(train_size)\n",
        "\n",
        "  return train_dataset,val_dataset"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TWDrV9ZeBKRJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#create training, validation and test dataset\n",
        "train_dataset, val_dataset = slice_dataset(dataset, 0.8)\n",
        "print('size of train_dataset:',len(list(train_dataset)))\n",
        "print('size of val_dataset:',len(list(val_dataset)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DSTkXW9WkMyy",
        "colab_type": "text"
      },
      "source": [
        "### 3.2.3 Parallelize Loading"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Uv100RJLVkl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "batch_size = 32\n",
        "train_dataset = train_dataset.cache()\n",
        "train_dataset = train_dataset.shuffle(1024).repeat().batch(batch_size,drop_remainder=True)\n",
        "train_dataset = train_dataset.prefetch(buffer_size = tf.data.experimental.AUTOTUNE)\n",
        "val_dataset = val_dataset.repeat().batch(batch_size, drop_remainder=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nU63IyGVDCJI",
        "colab_type": "text"
      },
      "source": [
        "#4. Train Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1VnYDT576ZCE",
        "colab_type": "text"
      },
      "source": [
        "##4.1 Build the model and find the optimal learning rate"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sDkNzoVK6Ui-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#clear history if necessary\n",
        "tf.keras.backend.clear_session()\n",
        "\n",
        "#train model\n",
        "learning_rate = 0.0001\n",
        "strategy = tf.distribute.MirroredStrategy(cross_device_ops=tf.distribute.HierarchicalCopyAllReduce()) ##to overwrite NCCL cross device communication as this is running in Windows\n",
        "with strategy.scope():\n",
        "\n",
        "  model = tf.keras.Sequential([\n",
        "      #1st Conv2D\n",
        "      tf.keras.layers.Conv2D(8, (1, 1), strides=(1, 1), \n",
        "                            activation='relu', input_shape=(1,9000,1)),\n",
        "      tf.keras.layers.BatchNormalization(),\n",
        "      tf.keras.layers.MaxPooling2D(pool_size=(1, 2),strides=(1, 2)),\n",
        "      tf.keras.layers.Dropout(0.2),\n",
        "      #2nd Conv2D\n",
        "      tf.keras.layers.Conv2D(16, (1, 3), strides=(1, 1),\n",
        "                            activation='relu'),\n",
        "      tf.keras.layers.BatchNormalization(),\n",
        "      tf.keras.layers.MaxPooling2D(pool_size=(1, 2),strides=(1, 2)),\n",
        "      tf.keras.layers.Dropout(0.2),\n",
        "      #3rd Conv2D\n",
        "      tf.keras.layers.Conv2D(32, (1, 3), strides=(1, 1),\n",
        "                            activation='relu'),\n",
        "      tf.keras.layers.BatchNormalization(),\n",
        "      tf.keras.layers.MaxPooling2D(pool_size=(1, 2),strides=(1, 2)),\n",
        "      tf.keras.layers.Dropout(0.2),\n",
        "      #4th Conv2D\n",
        "      tf.keras.layers.Conv2D(64, (1, 3), strides=(1, 1),\n",
        "                            activation='relu'),\n",
        "      tf.keras.layers.BatchNormalization(),\n",
        "      tf.keras.layers.MaxPooling2D(pool_size=(1, 2),strides=(1, 2)),\n",
        "      tf.keras.layers.Dropout(0.2),\n",
        "      #5th Conv2D\n",
        "      tf.keras.layers.Conv2D(16, (1, 1), strides=(1, 1),\n",
        "                            activation='relu'),\n",
        "      tf.keras.layers.BatchNormalization(),\n",
        "      #Full connection layer\n",
        "      tf.keras.layers.Flatten(),\n",
        "      tf.keras.layers.Dense(64, activation='relu'),\n",
        "      tf.keras.layers.BatchNormalization(),\n",
        "      tf.keras.layers.Dropout(0.2),\n",
        "      tf.keras.layers.Dense(1, activation = 'sigmoid')\n",
        "  ])\n",
        "\n",
        "model.summary()\n",
        "\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(lr=learning_rate), \n",
        "              loss=tf.keras.losses.binary_crossentropy, \n",
        "              metrics=['accuracy'])\n",
        "\n",
        "#callback: schedule a learning rate incline iteration\n",
        "lr_schedule = tf.keras.callbacks.LearningRateScheduler(\n",
        "    lambda epoch: 1e-4 * 10**(epoch / 20))\n",
        "\n",
        "#callback: tensorboard\n",
        "log_dir=\"logs\\\\fit\\\\\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "tensorboard_callback = TensorBoard(log_dir=log_dir, update_freq='batch', histogram_freq=1)\n",
        "\n",
        "#start training\n",
        "history = model.fit(train_dataset,\n",
        "                    epochs=50,\n",
        "                    steps_per_epoch = len(list(dataset))*0.8/batch_size,\n",
        "                    verbose=0,\n",
        "                    validation_data=val_dataset,\n",
        "                    validation_steps = len(list(dataset))*0.1/batch_size,\n",
        "                    callbacks=[lr_schedule,tensorboard_callback]\n",
        "                    )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZQsviSOFP-uc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Visualize learning rate vs epoches\n",
        "plt.semilogx(history.history[\"lr\"], history.history[\"loss\"])\n",
        "plt.axis([1e-4, 1e-2,0,5])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZwW8RHye6jBS",
        "colab_type": "text"
      },
      "source": [
        "##4.2 Train the model with the optimal learning rate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h6yI-nje0P1q",
        "colab_type": "text"
      },
      "source": [
        "###4.2.1 Create a Confusion Matrix callback"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ny7egL6l7iX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def plot_to_image(figure):\n",
        "    \"\"\"\n",
        "    Converts the matplotlib plot specified by 'figure' to a PNG image and\n",
        "    returns it. The supplied figure is closed and inaccessible after this call.\n",
        "    \"\"\"\n",
        "    \n",
        "    buf = io.BytesIO()\n",
        "    \n",
        "    # EXERCISE: Use plt.savefig to save the plot to a PNG in memory.\n",
        "    # YOUR CODE HERE\n",
        "    plt.savefig(buf, format='png')\n",
        "    # Closing the figure prevents it from being displayed directly inside\n",
        "    # the notebook.\n",
        "    plt.close(figure)\n",
        "    buf.seek(0)\n",
        "    \n",
        "    # EXERCISE: Use tf.image.decode_png to convert the PNG buffer\n",
        "    # to a TF image. Make sure you use 4 channels.\n",
        "    image = tf.image.decode_png(buf.getvalue(), channels=4)\n",
        "    \n",
        "    # EXERCISE: Use tf.expand_dims to add the batch dimension\n",
        "    image = tf.expand_dims(image,0)\n",
        "    \n",
        "    return image"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6GJf2f1z0Hry",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class_names = ['NO Afib','Afib']\n",
        "#https://axbihaqixpqbrrxincyxja.coursera-apps.org/notebooks/week3/TF_Serving_Week_3_Exercise_Question.ipynb\n",
        "\n",
        "def plot_confusion_matrix(cm, class_names):\n",
        "    \"\"\"\n",
        "    Returns a matplotlib figure containing the plotted confusion matrix.\n",
        "    \n",
        "    Args:\n",
        "       cm (array, shape = [n, n]): a confusion matrix of integer classes\n",
        "       class_names (array, shape = [n]): String names of the integer classes\n",
        "    \"\"\"\n",
        "    \n",
        "    figure = plt.figure(figsize=(8, 8))\n",
        "    plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
        "    plt.title(\"Confusion matrix\")\n",
        "    plt.colorbar()\n",
        "    tick_marks = np.arange(len(class_names))\n",
        "    plt.xticks(tick_marks, class_names, rotation=45)\n",
        "    plt.yticks(tick_marks, class_names)\n",
        "    \n",
        "    # Normalize the confusion matrix.\n",
        "    cm = np.around(cm.astype('float') / cm.sum(axis=1)[:, np.newaxis], decimals=2)\n",
        "    \n",
        "    # Use white text if squares are dark; otherwise black.\n",
        "    threshold = cm.max() / 2.\n",
        "    \n",
        "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
        "      color = \"white\" if cm[i, j] > threshold else \"black\"\n",
        "      plt.text(j, i, cm[i, j], horizontalalignment=\"center\", verticalalignment='center', color=color)\n",
        "        \n",
        "    plt.tight_layout()\n",
        "    plt.ylabel('True label')\n",
        "    plt.xlabel('Predicted label')\n",
        "    return figure"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "idsP-pwr0q3A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def log_confusion_matrix(epoch, logs):\n",
        "    \n",
        "    # EXERCISE: Use the model to predict the values from the test_images.\n",
        "    test_pred_raw = model.predict(test_signals)\n",
        "    \n",
        "    test_pred = np.where(test_pred_raw > 0.5, 1, 0)\n",
        "    \n",
        "    # EXERCISE: Calculate the confusion matrix using sklearn.metrics\n",
        "    cm = sklearn.metrics.confusion_matrix(test_labels, test_pred)\n",
        "    \n",
        "    figure = plot_confusion_matrix(cm, class_names=class_names)\n",
        "    cm_image = plot_to_image(figure)\n",
        "    \n",
        "    # Log the confusion matrix as an image summary.\n",
        "    with file_writer_cm.as_default():\n",
        "        tf.summary.image(\"Confusion Matrix\", cm_image, step=epoch)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MEmb8aS60tLW",
        "colab_type": "text"
      },
      "source": [
        "### 4.2.2 Start Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VWKdHVBBdKYE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%capture\n",
        "#clear history if necessary\n",
        "tf.keras.backend.clear_session()\n",
        "\n",
        "#train model\n",
        "learning_rate = 0.003 #choose the optimal learning rate\n",
        "strategy = tf.distribute.MirroredStrategy(cross_device_ops=tf.distribute.HierarchicalCopyAllReduce()) ##to overwrite NCCL cross device communication as this is running in Windows\n",
        "with strategy.scope():\n",
        "\n",
        "  model = tf.keras.Sequential([\n",
        "      #1st Conv2D\n",
        "      tf.keras.layers.Conv2D(8, (1, 1), strides=(1, 1), \n",
        "                            activation='relu', input_shape=(1,9000,1)),\n",
        "      tf.keras.layers.BatchNormalization(),\n",
        "      tf.keras.layers.MaxPooling2D(pool_size=(1, 2),strides=(1, 2)),\n",
        "      tf.keras.layers.Dropout(0.2),\n",
        "      #2nd Conv2D\n",
        "      tf.keras.layers.Conv2D(16, (1, 3), strides=(1, 1),\n",
        "                            activation='relu'),\n",
        "      tf.keras.layers.BatchNormalization(),\n",
        "      tf.keras.layers.MaxPooling2D(pool_size=(1, 2),strides=(1, 2)),\n",
        "      tf.keras.layers.Dropout(0.2),\n",
        "      #3rd Conv2D\n",
        "      tf.keras.layers.Conv2D(32, (1, 3), strides=(1, 1),\n",
        "                            activation='relu'),\n",
        "      tf.keras.layers.BatchNormalization(),\n",
        "      tf.keras.layers.MaxPooling2D(pool_size=(1, 2),strides=(1, 2)),\n",
        "      tf.keras.layers.Dropout(0.2),\n",
        "      #4th Conv2D\n",
        "      tf.keras.layers.Conv2D(64, (1, 3), strides=(1, 1),\n",
        "                            activation='relu'),\n",
        "      tf.keras.layers.BatchNormalization(),\n",
        "      tf.keras.layers.MaxPooling2D(pool_size=(1, 2),strides=(1, 2)),\n",
        "      tf.keras.layers.Dropout(0.2),\n",
        "      #5th Conv2D\n",
        "      tf.keras.layers.Conv2D(16, (1, 1), strides=(1, 1),\n",
        "                            activation='relu'),\n",
        "      tf.keras.layers.BatchNormalization(),\n",
        "      #Full connection layer\n",
        "      tf.keras.layers.Flatten(),\n",
        "      tf.keras.layers.Dense(64, activation='relu'),\n",
        "      tf.keras.layers.BatchNormalization(),\n",
        "      tf.keras.layers.Dropout(0.2),\n",
        "      tf.keras.layers.Dense(1, activation = 'sigmoid')\n",
        "  ])\n",
        "\n",
        "model.summary()\n",
        "\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(lr=learning_rate), \n",
        "              loss=tf.keras.losses.binary_crossentropy, \n",
        "              metrics=['accuracy'])\n",
        "\n",
        "#callback: tensorboard\n",
        "log_dir=\"logs\\\\fit\\\\\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "file_writer_cm = tf.summary.create_file_writer(log_dir + '/cm')\n",
        "\n",
        "cm_callback = keras.callbacks.LambdaCallback(on_epoch_end=log_confusion_matrix)\n",
        "tensorboard_callback = TensorBoard(log_dir=log_dir, update_freq='batch', profile_batch=1, histogram_freq=1)\n",
        "\n",
        "#start training\n",
        "history = model.fit(train_dataset,\n",
        "                    epochs=200,\n",
        "                    steps_per_epoch = len(list(dataset))*0.8/batch_size,\n",
        "                    verbose=0,\n",
        "                    validation_data=val_dataset,\n",
        "                    validation_steps = len(list(dataset))*0.1/batch_size,\n",
        "                    callbacks=[tensorboard_callback, cm_callback]\n",
        "                    );"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QsZzsnHSmqsu",
        "colab_type": "text"
      },
      "source": [
        "# 5. Model Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kOPNrWOSmyVC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Confusion Matrix with different threshold"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y8NLC_r9_OlK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "threshold = 0.2\n",
        "test_pred_raw = model.predict(test_signals)\n",
        "test_pred = np.where(test_pred_raw > threshold, 1, 0)\n",
        "    \n",
        "# EXERCISE: Calculate the confusion matrix using sklearn.metrics\n",
        "cm = sklearn.metrics.confusion_matrix(test_labels, test_pred)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "baORhWUe_ZLF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(cm)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GxmtvXz7UFe4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import plot_confusion_matrix as pcm"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tRf0N-vHRPc1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "figure = plot_confusion_matrix(cm, class_names=class_names)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pcu2_YJUbHGB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "report = sklearn.metrics.classification_report(test_labels, test_pred)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tvq1DoPvQYxU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(report)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aFGzdn-5QZnS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "score = sklearn.metrics.roc_auc_score(test_labels, test_pred)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nv8nmSe_Q4W6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(score)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "re4CuTxtQ5dS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}