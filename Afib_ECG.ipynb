{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Afib_ECG.ipynb",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/supertime1/Afib_PPG/blob/master/Afib_ECG.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-oNUq1dRJq-r",
        "colab_type": "text"
      },
      "source": [
        "#1.Introduction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DIS5QlL3G0G7",
        "colab_type": "text"
      },
      "source": [
        "This notebook trains an ECG DNN by using labeled ECG data from \"The PhysioNet Computing in Cardiology Challenge 2017\" (https://physionet.org/content/challenge-2017/1.0.0/). The ECG DNN model will be used to label the ECG data from MIMIC-III waveform dataset, so as the concurrent PPG data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jSibYKc0KaNC",
        "colab_type": "text"
      },
      "source": [
        "The ECG data used in training and validation has the following important attributes:\n",
        "*   sampling frequency: 300Hz\n",
        "*   4 lables: Normal (N), AF (A), Other rhythm (O), Noisy (~)\n",
        "*   length: 9 - 60s with 30s mean.\n",
        "*   preprosessed: data has been band pass filtered by AliveCor device\n",
        "\n",
        "\n",
        "Only time length >30s is used in training, since PPG data usually requires 30s for Afib detection."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w9YY9bo0CLNy",
        "colab_type": "text"
      },
      "source": [
        "#2.Setup Environment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xa66CINNKj1y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install tensorflow-gpu==2.0.0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q95xF5tmemik",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install wfdb"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NGo63wdRGY03",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from IPython.display import display\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import numpy as np\n",
        "import os\n",
        "import shutil\n",
        "import glob\n",
        "import wfdb\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.models import load_model \n",
        "print(tf.__version__)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UsKVuNWV6t4v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#run this cell if multiple GPUs are used\n",
        "tf.debugging.set_log_device_placement(True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZyD6ZPEDKyYw",
        "colab_type": "text"
      },
      "source": [
        "#3.Data Pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fkIhig4MCdvc",
        "colab_type": "text"
      },
      "source": [
        "##3.1 Load data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Totv4fFHKwED",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "hd_names = []\n",
        "for name in glob.glob('/content/drive/My Drive/training2017/*.hea'): #(\"C:/Users/57lzhang.US04WW4008/Desktop/Afib/Afib_ECG data/training2017/*.hea\"):#'/content/drive/My Drive/training2017/*.hea'): \n",
        "  position = name.index('.hea')\n",
        "  name = name[0:position] #remove the .hea part to comply with the wfdb.rdrecord format\n",
        "  hd_names.append(name)\n",
        "print('There are total', len(hd_names), 'records')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "50P04jUzLgZG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "qualified_names = [] #a list of file names that contain ECG Lead I data\n",
        "for name in hd_names[:256]:\n",
        "  record = wfdb.rdheader(name)\n",
        "  if record.sig_len >= 9000: #extact only records contrains ECG lead I >30s\n",
        "    qualified_names.append(name)\n",
        "print('There are total', len(qualified_names), 'qualified (>30s) records')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "39lu0EfVOxso",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#load label numpy file\n",
        "df = pd.read_csv('/content/drive/My Drive/training2017/REFERENCE.csv',sep=',', header=None)#(r'C:\\Users\\57lzhang.US04WW4008\\Desktop\\Afib\\Afib_ECG data\\training2017\\REFERENCE.csv', sep=',', header=None) #'/content/drive/My Drive/training2017/REFERENCE.csv',sep=',', header=None)#\n",
        "#create a new name list that only stores the file name\n",
        "new_name =[]\n",
        "for name in qualified_names:\n",
        "  new_name.append(name[-6:])\n",
        "len(new_name)\n",
        "\n",
        "init_labels = df[df[0].isin(new_name)][1].to_numpy()\n",
        "init_labels[init_labels == 'N'] = 0\n",
        "init_labels[init_labels == 'A'] = 1\n",
        "init_labels[init_labels == 'O'] = 2\n",
        "init_labels[init_labels == '~'] = 3"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ribTthakZcx3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "##read signals\n",
        "ECG_signals = [] #create a  list to store all  ECG signals\n",
        "for name in qualified_names:\n",
        "  record = wfdb.rdrecord(name)\n",
        "  ECG_signals.append(record.p_signal)\n",
        "\n",
        "print('ECG signals len:', len(ECG_signals))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "koI2rxjvaCAv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#A function to split the raw signal data with 30s per segment and keep the label\n",
        "##source is the raw signal (e.g. ECG_signals) and seg_len = 30s * 300Hz = 9000\n",
        "def generate_segment_data(source,init_labels,seg_len):\n",
        "  n=0\n",
        "  signals =[]\n",
        "  labels = []\n",
        "  for signal in source:\n",
        "    for i in range(int(len(signal)/seg_len)):\n",
        "      seg = signal[seg_len*i:seg_len*(i+1)]\n",
        "      label = init_labels[n]\n",
        "      signals.append(seg)\n",
        "      labels.append(label)\n",
        "  n+=1\n",
        "#convert list into a numpy array and change its dim from (num of records, seg_len, 1) to (num of records, seg_len)\n",
        "  signals = np.asarray(list(map(lambda x: np.reshape(x,9000),signals)))\n",
        "  labels = np.asarray(list(map(lambda x: np.reshape(x,1),labels)))\n",
        "\n",
        "  return signals,labels"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zVL-5Pxdc7C5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#use generate_segment_data() to generate segments with labels\n",
        "#After segmentation, more data than previous is generated, because some source data are 60s \n",
        "signals, labels = generate_segment_data(ECG_signals,init_labels,9000)\n",
        "print('signals dim:',signals.shape)\n",
        "print('labels dim:',labels.shape)\n",
        "#resize the dimension for use in CNN\n",
        "signals = np.expand_dims(signals,axis=1)\n",
        "signals = np.expand_dims(signals,axis=3)\n",
        "print('signals dim after resize:',signals.shape)\n",
        "print('labels dim after resize:',labels.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MkBe67QpCuEf",
        "colab_type": "text"
      },
      "source": [
        "##3.2 Convert to tf.data format and Slice dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yb-w63Qfbo6u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#create tf.data.dataset from np.array\n",
        "dataset = tf.data.Dataset.from_tensor_slices((signals,labels))\n",
        "#dataset = dataset.shuffle(len(signals))\n",
        "#dataset = dataset.map(train_preprocess, num_parallel_calls=4)\n",
        "#dataset = dataset.batch(batch_size=32)\n",
        "#dataset = dataset.prefetch(1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cVydDYW0Hhqn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dataset"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mmT2oMdPM6ed",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "labels = tf.strings.to_number(labels)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TO8KaCDab4Yf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#optional: data preprocessing use dataset.map(function)\n",
        "#possible functions:\n",
        "#def train_preprocess(spectrogram = None, hample = None, flat_peak = None, normalization = None):\n",
        "#  if spectrogram != 0:\n",
        "  \n",
        "#  if hample != 0:\n",
        "#    ...\n",
        "#1.Spectrogram\n",
        "#2.Hample filter\n",
        "#3.Flat line and peak removal\n",
        "#4.Normalization\n",
        "#5..."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_WPHNzkD-baa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "###ratio value is between 0 and 1, and train,validation and test ratios should sum up to 1.0\n",
        "def slice_dataset(dataset,train_ratio,val_ratio,test_ratio=None):\n",
        "  DATASET_SIZE =len(list(dataset)) #only works in eager mode (e.g. TF version >= 2.0.x)\n",
        "  train_size = int(train_ratio * DATASET_SIZE)\n",
        "  val_size = int(val_ratio * DATASET_SIZE)\n",
        "  test_size = int(test_ratio * DATASET_SIZE)\n",
        "\n",
        "  train_dataset = dataset.take(train_size)\n",
        "  remain_dataset = dataset.skip(train_size)\n",
        "  val_dataset = remain_dataset.take(val_size)\n",
        "  test_dataset = remain_dataset.skip(val_size)\n",
        "\n",
        "  return train_dataset,val_dataset,test_dataset"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TWDrV9ZeBKRJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#create training, validation and test dataset\n",
        "train_dataset, val_dataset, test_dataset = slice_dataset(dataset, 0.8, 0.1, 0.1)\n",
        "print('size of train_dataset:',len(list(train_dataset)))\n",
        "print('size of val_dataset:',len(list(val_dataset)))\n",
        "print('size of test_dataset:',len(list(test_dataset)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Uv100RJLVkl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_dataset = train_dataset.shuffle(100).repeat().batch(32)\n",
        "val_dataset = val_dataset.repeat().batch(16)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nU63IyGVDCJI",
        "colab_type": "text"
      },
      "source": [
        "#4. Create Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1VnYDT576ZCE",
        "colab_type": "text"
      },
      "source": [
        "##4.1 Build the model and find the optimal learning rate"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sDkNzoVK6Ui-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# tensorflow input shape = (m,1,257,4)\n",
        "# Better to experiment with different filter size and strides\n",
        "## Experiment with batch normalization to see if it improve accuracy\n",
        "#clear history if necessary\n",
        "tf.keras.backend.clear_session()\n",
        "\n",
        "#train model\n",
        "learning_rate = 0.0001\n",
        "strategy = tf.distribute.MirroredStrategy(cross_device_ops=tf.distribute.HierarchicalCopyAllReduce()) ##to overwrite NCCL cross device communication as this is running in Windows\n",
        "with strategy.scope():\n",
        "\n",
        "  model = tf.keras.Sequential([\n",
        "      #1st Conv2D\n",
        "      tf.keras.layers.Conv2D(8, (1, 1), strides=(1, 1), \n",
        "                            activation='relu', input_shape=(1,9000,1)),\n",
        "      tf.keras.layers.BatchNormalization(),\n",
        "      tf.keras.layers.MaxPooling2D(pool_size=(1, 2),strides=(1, 2)),\n",
        "      tf.keras.layers.Dropout(0.2),\n",
        "      #2nd Conv2D\n",
        "      tf.keras.layers.Conv2D(16, (1, 3), strides=(1, 1),\n",
        "                            activation='relu'),\n",
        "      tf.keras.layers.BatchNormalization(),\n",
        "      tf.keras.layers.MaxPooling2D(pool_size=(1, 2),strides=(1, 2)),\n",
        "      tf.keras.layers.Dropout(0.2),\n",
        "      #3rd Conv2D\n",
        "      tf.keras.layers.Conv2D(32, (1, 3), strides=(1, 1),\n",
        "                            activation='relu'),\n",
        "      tf.keras.layers.BatchNormalization(),\n",
        "      tf.keras.layers.MaxPooling2D(pool_size=(1, 2),strides=(1, 2)),\n",
        "      tf.keras.layers.Dropout(0.2),\n",
        "      #4th Conv2D\n",
        "      tf.keras.layers.Conv2D(64, (1, 3), strides=(1, 1),\n",
        "                            activation='relu'),\n",
        "      tf.keras.layers.BatchNormalization(),\n",
        "      tf.keras.layers.MaxPooling2D(pool_size=(1, 2),strides=(1, 2)),\n",
        "      tf.keras.layers.Dropout(0.2),\n",
        "      #5th Conv2D\n",
        "      tf.keras.layers.Conv2D(16, (1, 1), strides=(1, 1),\n",
        "                            activation='relu'),\n",
        "      tf.keras.layers.BatchNormalization(),\n",
        "      #Full connection layer\n",
        "      tf.keras.layers.Flatten(),\n",
        "      tf.keras.layers.Dense(64, activation='relu'),\n",
        "      tf.keras.layers.BatchNormalization(),\n",
        "      tf.keras.layers.Dropout(0.2),\n",
        "      tf.keras.layers.Dense(1)\n",
        "  ])\n",
        "  model.summary()\n",
        "  model.compile(optimizer=tf.keras.optimizers.Adam(lr=learning_rate), \n",
        "                loss=tf.keras.losses.categorical_crossentropy, \n",
        "                metrics=['accuracy'])\n",
        "\n",
        "#schedule a learning rate incline iteration\n",
        "lr_schedule = tf.keras.callbacks.LearningRateScheduler(\n",
        "    lambda epoch: 1e-4 * 10**(epoch / 20))\n",
        "\n",
        "#start training\n",
        "history = model.fit(train_dataset,\n",
        "                    epochs=50,\n",
        "                    step_per_epoch = len(list(train_dataset))/32,\n",
        "                    verbose=1,\n",
        "                    validation_data=val_dataset,\n",
        "                    validation_steps=len(list(val_dataset))/16,\n",
        "                    callbacks=[lr_schedule]\n",
        "                  )\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZQsviSOFP-uc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Visualize learning rate vs epoches\n",
        "plt.semilogx(history.history[\"lr\"], history.history[\"loss\"])\n",
        "plt.axis([1e-4, 1e-1,0,1000])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZwW8RHye6jBS",
        "colab_type": "text"
      },
      "source": [
        "##4.2 Train the model with the optimal learning rate"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VWKdHVBBdKYE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "learning_rate = 0.0005 ##this value is learned from previous training\n",
        "strategy = tf.distribute.MirroredStrategy(cross_device_ops=tf.distribute.HierarchicalCopyAllReduce()) ##to overwrite NCCL cross device communication as this is running in Windows\n",
        "with strategy.scope():\n",
        "\n",
        "  model = tf.keras.Sequential([\n",
        "      #1st Conv2D\n",
        "      tf.keras.layers.Conv2D(8, (1, 1), strides=(1, 1), \n",
        "                            activation='relu', input_shape=(1,257,4)),\n",
        "      tf.keras.layers.BatchNormalization(),\n",
        "      tf.keras.layers.MaxPooling2D(pool_size=(1, 2),strides=(1, 2)),\n",
        "      tf.keras.layers.Dropout(0.2),\n",
        "      #2nd Conv2D\n",
        "      tf.keras.layers.Conv2D(16, (1, 3), strides=(1, 1),\n",
        "                            activation='relu'),\n",
        "      tf.keras.layers.BatchNormalization(),\n",
        "      tf.keras.layers.MaxPooling2D(pool_size=(1, 2),strides=(1, 2)),\n",
        "      tf.keras.layers.Dropout(0.2),\n",
        "      #3rd Conv2D\n",
        "      tf.keras.layers.Conv2D(32, (1, 3), strides=(1, 1),\n",
        "                            activation='relu'),\n",
        "      tf.keras.layers.BatchNormalization(),\n",
        "      tf.keras.layers.MaxPooling2D(pool_size=(1, 2),strides=(1, 2)),\n",
        "      tf.keras.layers.Dropout(0.2),\n",
        "      #4th Conv2D\n",
        "      tf.keras.layers.Conv2D(64, (1, 3), strides=(1, 1),\n",
        "                            activation='relu'),\n",
        "      tf.keras.layers.BatchNormalization(),\n",
        "      tf.keras.layers.MaxPooling2D(pool_size=(1, 2),strides=(1, 2)),\n",
        "      tf.keras.layers.Dropout(0.2),\n",
        "      #5th Conv2D\n",
        "      tf.keras.layers.Conv2D(16, (1, 1), strides=(1, 1),\n",
        "                            activation='relu'),\n",
        "      tf.keras.layers.BatchNormalization(),\n",
        "      #Full connection layer\n",
        "      tf.keras.layers.Flatten(),\n",
        "      tf.keras.layers.Dense(64, activation='relu'),\n",
        "      tf.keras.layers.BatchNormalization(),\n",
        "      tf.keras.layers.Dropout(0.2),\n",
        "      tf.keras.layers.Dense(1)\n",
        "  ])\n",
        "\n",
        "  model.compile(optimizer=tf.keras.optimizers.Adam(lr=learning_rate), loss='mse', metrics=['mae'])\n",
        "\n",
        "#starting training\n",
        "history = model.fit(train_dataset,\n",
        "                    steps_per_epoch=len(training_set)/32,\n",
        "                    epochs=200,\n",
        "                    verbose=1,\n",
        "                    validation_data=validation_datagen.flow(validation_set,validation_label_set,batch_size=32),\n",
        "                    validation_steps=len(validation_set)/32,\n",
        "                   )"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}