{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Afib_ECG.ipynb",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/supertime1/Afib_PPG/blob/master/Afib_ECG.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-oNUq1dRJq-r",
        "colab_type": "text"
      },
      "source": [
        "#1.Introduction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DIS5QlL3G0G7",
        "colab_type": "text"
      },
      "source": [
        "This notebook trains an ECG DNN by using labeled ECG data from \"The PhysioNet Computing in Cardiology Challenge 2017\" (https://physionet.org/content/challenge-2017/1.0.0/). The ECG DNN model will be used to label the ECG data from MIMIC-III waveform dataset, so as the concurrent PPG data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jSibYKc0KaNC",
        "colab_type": "text"
      },
      "source": [
        "The ECG data used in training and validation has the following important attributes:\n",
        "*   sampling frequency: 300Hz\n",
        "*   4 lables: Normal (N), AF (A), Other rhythm (O), Noisy (~)\n",
        "*   length: 9 - 60s with 30s mean.\n",
        "*   preprosessed: data has been band pass filtered by AliveCor device\n",
        "\n",
        "\n",
        "Only time length >30s is used in training, since PPG data usually requires 30s for Afib detection."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w9YY9bo0CLNy",
        "colab_type": "text"
      },
      "source": [
        "#2.Setup Environment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xa66CINNKj1y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install tensorflow-gpu"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q95xF5tmemik",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install wfdb"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GJnlylcLwF5x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install tensorflow_datasets"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NGo63wdRGY03",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from IPython.display import display\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "%load_ext tensorboard\n",
        "import numpy as np\n",
        "import os\n",
        "import shutil\n",
        "import glob\n",
        "import wfdb\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.models import load_model \n",
        "from tensorflow.keras.callbacks import TensorBoard\n",
        "import tensorflow_datasets as tfds\n",
        "import multiprocessing\n",
        "from datetime import datetime\n",
        "\n",
        "print(tf.__version__)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UsKVuNWV6t4v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#run this cell if multiple GPUs are used\n",
        "tf.debugging.set_log_device_placement(True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xia3EgQ6-gNN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.python.client import device_lib \n",
        "print(device_lib.list_local_devices())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FuQ_w0-N-nrc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tf.test.is_built_with_cuda()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZyD6ZPEDKyYw",
        "colab_type": "text"
      },
      "source": [
        "#3.Data Pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fkIhig4MCdvc",
        "colab_type": "text"
      },
      "source": [
        "##3.1 Load data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Totv4fFHKwED",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "hd_names = []\n",
        "for name in glob.glob(\"C:/Users/57lzhang.US04WW4008/Desktop/Afib/Afib_ECG data/training2017/*.hea\"):#'/content/drive/My Drive/training2017/*.hea'): \n",
        "  position = name.index('.hea')\n",
        "  name = name[0:position] #remove the .hea part to comply with the wfdb.rdrecord format\n",
        "  hd_names.append(name)\n",
        "print('There are total', len(hd_names), 'records')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "50P04jUzLgZG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "qualified_names = [] #a list of file names that contain ECG Lead I data\n",
        "for name in hd_names:\n",
        "  record = wfdb.rdheader(name)\n",
        "  if record.sig_len >= 9000: #extact only records contrains ECG lead I >30s\n",
        "    qualified_names.append(name)\n",
        "print('There are total', len(qualified_names), 'qualified (>30s) records')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "39lu0EfVOxso",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#load label numpy file\n",
        "df = pd.read_csv(r'C:\\Users\\57lzhang.US04WW4008\\Desktop\\Afib\\Afib_ECG data\\training2017\\REFERENCE.csv', sep=',', header=None) #'/content/drive/My Drive/training2017/REFERENCE.csv',sep=',', header=None)#\n",
        "#create a new name list that only stores the file name\n",
        "init_labels =[]\n",
        "new_names = []\n",
        "for name in qualified_names:\n",
        "  temp_name = name[-6:] #remove the dir and keep only the file name\n",
        "  temp_label = df[df[0] == temp_name][1].to_numpy()\n",
        "  if temp_label != '~':\n",
        "    init_labels.append(temp_label)\n",
        "    new_names.append(name)\n",
        "init_labels = np.array(init_labels)\n",
        "print('There are total', len(init_labels),'none-nosiy labels')\n",
        "print('There are total', len(new_names), 'none-nosiy records')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uL4DhIRqGCjr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "init_labels[init_labels =='N'] = '0'\n",
        "init_labels[init_labels =='A'] = '1'\n",
        "init_labels[init_labels =='O'] = '0'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ribTthakZcx3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "##read signals\n",
        "ECG_signals = [] #create a  list to store all  ECG signals\n",
        "for name in new_names:\n",
        "  record = wfdb.rdrecord(name)\n",
        "  ECG_signals.append(record.p_signal)\n",
        "\n",
        "print('ECG signals len:', len(ECG_signals))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "koI2rxjvaCAv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#A function to split the raw signal data with 30s per segment and keep the label\n",
        "##source is the raw signal (e.g. ECG_signals) and seg_len = 30s * 300Hz = 9000\n",
        "def generate_segment_data(source,init_labels,seg_len):\n",
        "  n=0\n",
        "  signals =[]\n",
        "  labels = []\n",
        "  for signal in source:\n",
        "    for i in range(int(len(signal)/seg_len)):\n",
        "      seg = signal[seg_len*i:seg_len*(i+1)]\n",
        "      label = init_labels[n]\n",
        "      signals.append(seg)\n",
        "      labels.append(label)\n",
        "  n+=1\n",
        "#convert list into a numpy array and change its dim from (num of records, seg_len, 1) to (num of records, seg_len)\n",
        "  signals = np.asarray(list(map(lambda x: np.reshape(x,9000),signals)))\n",
        "  labels = np.asarray(list(map(lambda x: np.reshape(x,1),labels)))\n",
        "\n",
        "  return signals,labels"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zVL-5Pxdc7C5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#use generate_segment_data() to generate segments with labels\n",
        "#After segmentation, more data than previous is generated, because some source data are 60s \n",
        "signals, labels = generate_segment_data(ECG_signals,init_labels,9000)\n",
        "print('signals dim:',signals.shape)\n",
        "print('labels dim:',labels.shape)\n",
        "#resize the dimension for use in CNN\n",
        "signals = np.expand_dims(signals,axis=1)\n",
        "signals = np.expand_dims(signals,axis=3)\n",
        "print('signals dim after resize:',signals.shape)\n",
        "print('labels dim after resize:',labels.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MkBe67QpCuEf",
        "colab_type": "text"
      },
      "source": [
        "##3.2 Extract, Transform and Load data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RBWlyLWskq8C",
        "colab_type": "text"
      },
      "source": [
        "###3.2.1 Parallelize Extraction (how?)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yb-w63Qfbo6u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "labels = tf.strings.to_number(labels) \n",
        "dataset = tf.data.Dataset.from_tensor_slices((signals,labels))\n",
        "dataset"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NlxfJjfzffG3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#print and visualize the data\n",
        "for signal, label in tfds.as_numpy(dataset.take(2)):\n",
        "  print(signal.shape, label)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TO8KaCDab4Yf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#optional: data preprocessing use dataset.map(function)\n",
        "#possible functions:\n",
        "#1.Spectrogram\n",
        "#2.Hample filter\n",
        "#3.Flat line and peak removal\n",
        "#4.Normalization"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J5wqFGtUkFwj",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "### 3.2.2 Parallelize Transformation\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "82EjsJdzjZF8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cores = multiprocessing.cpu_count()\n",
        "print(cores)\n",
        "#dataset = dataset.map(function, num_parallel_calls = cores)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_WPHNzkD-baa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "###ratio value is between 0 and 1, and train,validation and test ratios should sum up to 1.0\n",
        "def slice_dataset(dataset,train_ratio,val_ratio,test_ratio=None):\n",
        "  DATASET_SIZE =len(list(dataset)) #only works in eager mode (e.g. TF version >= 2.0.x)\n",
        "  train_size = int(train_ratio * DATASET_SIZE)\n",
        "  val_size = int(val_ratio * DATASET_SIZE)\n",
        "  test_size = int(test_ratio * DATASET_SIZE)\n",
        "\n",
        "  train_dataset = dataset.take(train_size)\n",
        "  remain_dataset = dataset.skip(train_size)\n",
        "  val_dataset = remain_dataset.take(val_size)\n",
        "  test_dataset = remain_dataset.skip(val_size)\n",
        "\n",
        "  return train_dataset,val_dataset,test_dataset"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TWDrV9ZeBKRJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#create training, validation and test dataset\n",
        "train_dataset, val_dataset, test_dataset = slice_dataset(dataset, 0.8, 0.1, 0.1)\n",
        "print('size of train_dataset:',len(list(train_dataset)))\n",
        "print('size of val_dataset:',len(list(val_dataset)))\n",
        "print('size of test_dataset:',len(list(test_dataset)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DSTkXW9WkMyy",
        "colab_type": "text"
      },
      "source": [
        "### 3.2.3 Parallelize Loading"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Uv100RJLVkl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "batch_size = 32\n",
        "train_dataset = train_dataset.cache()\n",
        "train_dataset = train_dataset.shuffle(1024).repeat().batch(batch_size,drop_remainder=True)\n",
        "train_dataset = train_dataset.prefetch(buffer_size = tf.data.experimental.AUTOTUNE)\n",
        "val_dataset = val_dataset.repeat().batch(batch_size, drop_remainder=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nU63IyGVDCJI",
        "colab_type": "text"
      },
      "source": [
        "#4. Create Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1VnYDT576ZCE",
        "colab_type": "text"
      },
      "source": [
        "##4.1 Build the model and find the optimal learning rate"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sDkNzoVK6Ui-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#clear history if necessary\n",
        "tf.keras.backend.clear_session()\n",
        "\n",
        "#train model\n",
        "learning_rate = 0.0001\n",
        "strategy = tf.distribute.MirroredStrategy(cross_device_ops=tf.distribute.HierarchicalCopyAllReduce()) ##to overwrite NCCL cross device communication as this is running in Windows\n",
        "with strategy.scope():\n",
        "\n",
        "  model = tf.keras.Sequential([\n",
        "      #1st Conv2D\n",
        "      tf.keras.layers.Conv2D(8, (1, 1), strides=(1, 1), \n",
        "                            activation='relu', input_shape=(1,9000,1)),\n",
        "      tf.keras.layers.BatchNormalization(),\n",
        "      tf.keras.layers.MaxPooling2D(pool_size=(1, 2),strides=(1, 2)),\n",
        "      tf.keras.layers.Dropout(0.2),\n",
        "      #2nd Conv2D\n",
        "      tf.keras.layers.Conv2D(16, (1, 3), strides=(1, 1),\n",
        "                            activation='relu'),\n",
        "      tf.keras.layers.BatchNormalization(),\n",
        "      tf.keras.layers.MaxPooling2D(pool_size=(1, 2),strides=(1, 2)),\n",
        "      tf.keras.layers.Dropout(0.2),\n",
        "      #3rd Conv2D\n",
        "      tf.keras.layers.Conv2D(32, (1, 3), strides=(1, 1),\n",
        "                            activation='relu'),\n",
        "      tf.keras.layers.BatchNormalization(),\n",
        "      tf.keras.layers.MaxPooling2D(pool_size=(1, 2),strides=(1, 2)),\n",
        "      tf.keras.layers.Dropout(0.2),\n",
        "      #4th Conv2D\n",
        "      tf.keras.layers.Conv2D(64, (1, 3), strides=(1, 1),\n",
        "                            activation='relu'),\n",
        "      tf.keras.layers.BatchNormalization(),\n",
        "      tf.keras.layers.MaxPooling2D(pool_size=(1, 2),strides=(1, 2)),\n",
        "      tf.keras.layers.Dropout(0.2),\n",
        "      #5th Conv2D\n",
        "      tf.keras.layers.Conv2D(16, (1, 1), strides=(1, 1),\n",
        "                            activation='relu'),\n",
        "      tf.keras.layers.BatchNormalization(),\n",
        "      #Full connection layer\n",
        "      tf.keras.layers.Flatten(),\n",
        "      tf.keras.layers.Dense(64, activation='relu'),\n",
        "      tf.keras.layers.BatchNormalization(),\n",
        "      tf.keras.layers.Dropout(0.2),\n",
        "      tf.keras.layers.Dense(1)\n",
        "  ])\n",
        "\n",
        "model.summary()\n",
        "\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(lr=learning_rate), \n",
        "              loss=tf.keras.losses.binary_crossentropy, \n",
        "              metrics=['accuracy'])\n",
        "\n",
        "#callback: schedule a learning rate incline iteration\n",
        "lr_schedule = tf.keras.callbacks.LearningRateScheduler(\n",
        "    lambda epoch: 1e-4 * 10**(epoch / 20))\n",
        "\n",
        "#callback: tensorboard\n",
        "!rm -rf logs\\\\fit\n",
        "log_dir=\"logs\\\\fit\\\\\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "tensorboard_callback = TensorBoard(log_dir=log_dir, update_freq='batch', histogram_freq=1)\n",
        "\n",
        "#start training\n",
        "history = model.fit(train_dataset,\n",
        "                    epochs=1,\n",
        "                    steps_per_epoch = len(list(dataset))*0.8/batch_size,\n",
        "                    verbose=1,\n",
        "                    validation_data=val_dataset,\n",
        "                    validation_steps = len(list(dataset))*0.1/batch_size,\n",
        "                    callbacks=[lr_schedule,tensorboard_callback]\n",
        "                    )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZQsviSOFP-uc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Visualize learning rate vs epoches\n",
        "plt.semilogx(history.history[\"lr\"], history.history[\"loss\"])\n",
        "plt.axis([1e-4, 1e-2,0,5])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uGIC_8p0LI_R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!rm -rf logs\\\\fit\n",
        "log_dir=\"logs\\\\fit\\\\\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "%tensorboard --logdir log_dir"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZwW8RHye6jBS",
        "colab_type": "text"
      },
      "source": [
        "##4.2 Train the model with the optimal learning rate"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VWKdHVBBdKYE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#clear history if necessary\n",
        "tf.keras.backend.clear_session()\n",
        "\n",
        "#train model\n",
        "learning_rate = 0.001 #choose the optimal learning rate\n",
        "strategy = tf.distribute.MirroredStrategy(cross_device_ops=tf.distribute.HierarchicalCopyAllReduce()) ##to overwrite NCCL cross device communication as this is running in Windows\n",
        "with strategy.scope():\n",
        "\n",
        "  model = tf.keras.Sequential([\n",
        "      #1st Conv2D\n",
        "      tf.keras.layers.Conv2D(8, (1, 1), strides=(1, 1), \n",
        "                            activation='relu', input_shape=(1,9000,1)),\n",
        "      tf.keras.layers.BatchNormalization(),\n",
        "      tf.keras.layers.MaxPooling2D(pool_size=(1, 2),strides=(1, 2)),\n",
        "      tf.keras.layers.Dropout(0.2),\n",
        "      #2nd Conv2D\n",
        "      tf.keras.layers.Conv2D(16, (1, 3), strides=(1, 1),\n",
        "                            activation='relu'),\n",
        "      tf.keras.layers.BatchNormalization(),\n",
        "      tf.keras.layers.MaxPooling2D(pool_size=(1, 2),strides=(1, 2)),\n",
        "      tf.keras.layers.Dropout(0.2),\n",
        "      #3rd Conv2D\n",
        "      tf.keras.layers.Conv2D(32, (1, 3), strides=(1, 1),\n",
        "                            activation='relu'),\n",
        "      tf.keras.layers.BatchNormalization(),\n",
        "      tf.keras.layers.MaxPooling2D(pool_size=(1, 2),strides=(1, 2)),\n",
        "      tf.keras.layers.Dropout(0.2),\n",
        "      #4th Conv2D\n",
        "      tf.keras.layers.Conv2D(64, (1, 3), strides=(1, 1),\n",
        "                            activation='relu'),\n",
        "      tf.keras.layers.BatchNormalization(),\n",
        "      tf.keras.layers.MaxPooling2D(pool_size=(1, 2),strides=(1, 2)),\n",
        "      tf.keras.layers.Dropout(0.2),\n",
        "      #5th Conv2D\n",
        "      tf.keras.layers.Conv2D(16, (1, 1), strides=(1, 1),\n",
        "                            activation='relu'),\n",
        "      tf.keras.layers.BatchNormalization(),\n",
        "      #Full connection layer\n",
        "      tf.keras.layers.Flatten(),\n",
        "      tf.keras.layers.Dense(64, activation='relu'),\n",
        "      tf.keras.layers.BatchNormalization(),\n",
        "      tf.keras.layers.Dropout(0.2),\n",
        "      tf.keras.layers.Dense(1)\n",
        "  ])\n",
        "\n",
        "model.summary()\n",
        "\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(lr=learning_rate), \n",
        "              loss=tf.keras.losses.binary_crossentropy, \n",
        "              metrics=['accuracy'])\n",
        "\n",
        "#callback: tensorboard\n",
        "!rm -rf logs\\\\fit\n",
        "log_dir=\"logs\\\\fit\\\\\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "tensorboard_callback = TensorBoard(log_dir=log_dir, update_freq='batch', histogram_freq=1)\n",
        "\n",
        "#start training\n",
        "history = model.fit(train_dataset,\n",
        "                    epochs=1,\n",
        "                    steps_per_epoch = len(list(dataset))*0.8/batch_size,\n",
        "                    verbose=1,\n",
        "                    validation_data=val_dataset,\n",
        "                    validation_steps = len(list(dataset))*0.1/batch_size,\n",
        "                    callbacks=[tensorboard_callback]\n",
        "                    )"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}